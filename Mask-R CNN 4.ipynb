{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Annotations file not found at C:\\New folder\\Dr. Surya\\MaskRCNN\\Unity_Generation\\Concrete\\annotations.json.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CrackDataset' object has no attribute 'coco'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\New folder\\Dr. Surya\\MaskRCNN\\demo\\new.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/New%20folder/Dr.%20Surya/MaskRCNN/demo/new.ipynb#W1sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m dataset \u001b[39m=\u001b[39m CrackDataset(DATA_DIR)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/New%20folder/Dr.%20Surya/MaskRCNN/demo/new.ipynb#W1sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/New%20folder/Dr.%20Surya/MaskRCNN/demo/new.ipynb#W1sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m train_set, val_set \u001b[39m=\u001b[39m random_split(dataset, [\u001b[39mint\u001b[39m(\u001b[39m0.8\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39;49m(dataset)), \u001b[39mint\u001b[39m(\u001b[39m0.2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(dataset))])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/New%20folder/Dr.%20Surya/MaskRCNN/demo/new.ipynb#W1sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_set, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/New%20folder/Dr.%20Surya/MaskRCNN/demo/new.ipynb#W1sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(val_set, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mc:\\New folder\\Dr. Surya\\MaskRCNN\\demo\\new.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/New%20folder/Dr.%20Surya/MaskRCNN/demo/new.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/New%20folder/Dr.%20Surya/MaskRCNN/demo/new.ipynb#W1sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoco\u001b[39m.\u001b[39mimgs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CrackDataset' object has no attribute 'coco'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.ops import nms\n",
    "\n",
    "DATA_DIR = \"C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\"\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, \"Images\")\n",
    "MASKS_DIR = os.path.join(DATA_DIR, \"Masks\")\n",
    "ANNOTATIONS_DIR = os.path.join(DATA_DIR, \"BoundingBoxs\")\n",
    "\n",
    "\n",
    "def get_annotations_file_path(image_filename):\n",
    "    filename_without_extension = os.path.splitext(image_filename)[0]\n",
    "    return os.path.join(ANNOTATIONS_DIR, f\"{filename_without_extension}.json\")\n",
    "\n",
    "\n",
    "class CrackDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.annotations_path = os.path.join(root_dir, 'annotations.json')\n",
    "        self.bounding_box_dir = os.path.join(root_dir, 'BoundingBoxs')\n",
    "        self.load_annotations()\n",
    "\n",
    "    def load_annotations(self):\n",
    "        try:\n",
    "            self.coco = COCO(self.annotations_path)\n",
    "            print(\"Annotations loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Annotations file not found at {self.annotations_path}.\")\n",
    "\n",
    "    def load_bounding_boxes(self):\n",
    "        try:\n",
    "            bounding_box_files = os.listdir(self.bounding_box_dir)\n",
    "            bounding_boxes = []\n",
    "            for file in bounding_box_files:\n",
    "                if file.endswith('.txt'):\n",
    "                    with open(os.path.join(self.bounding_box_dir, file), 'r') as f:\n",
    "                        # Process the bounding box data here\n",
    "                        # For example, you can read the coordinates from the file and store them in a list\n",
    "                        # bounding_boxes.append(...)\n",
    "                        pass\n",
    "            print(\"Bounding boxes loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Bounding box directory not found at {self.bounding_box_dir}.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(idx)\n",
    "        target = self.load_targets(idx)\n",
    "        if self.transform:\n",
    "            img, target = self.transform(img, target)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco.imgs)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        # get image ID and file name from coco\n",
    "        img_id = self.coco.imgs[i]['id']\n",
    "        file_name = self.coco.imgs[i]['file_name']\n",
    "\n",
    "        # load and return image\n",
    "        img_path = os.path.join(self.root_dir, 'images', file_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        return img\n",
    "\n",
    "    def load_targets(self, img_id):\n",
    "        # get ground truth annotations for image i\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # parse bbox and mask from annotations\n",
    "        boxes = []\n",
    "        masks = []\n",
    "        for ann in annotations:\n",
    "            boxes.append(ann['bbox'])\n",
    "\n",
    "            # load binary mask from COCO polygon area\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "\n",
    "        # construct targets dict\n",
    "        targets = {'boxes': boxes, 'labels': [], 'masks': masks}\n",
    "        return targets\n",
    "\n",
    "\n",
    "dataset = CrackDataset(DATA_DIR)\n",
    "dataloader = DataLoader(dataset)\n",
    "train_set, val_set = random_split(dataset, [int(0.8 * len(dataset)), int(0.2 * len(dataset))])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=1)\n",
    "val_loader = DataLoader(val_set, batch_size=1)\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn()\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "\n",
    "def evaluate(val_loader, model):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(outputs[\"scores\"], targets[\"labels\"])\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for imgs, targets in train_loader:\n",
    "        preds = model(imgs)\n",
    "        \n",
    "        loss_fn_train = nn.MSELoss()\n",
    "        \n",
    "        loss_train_1st_part=loss_fn_train(preds[0],targets[0])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train_1st_part.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss_train_1st_part.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = evaluate(val_loader, model)\n",
    "    print(f'Epoch {epoch + 1}, Val Loss: {val_loss}')\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(num_classes=2)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Learning rate and optimizer\n",
    "lr = 0.005\n",
    "optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Define anchor sizes as a list of lists\n",
    "anchor_sizes = [[128, 128], [256, 256], [512, 512]]\n",
    "\n",
    "# Generate anchors for each size\n",
    "anchors = []\n",
    "for size in anchor_sizes:\n",
    "    # Generate anchors centered at reference point (0,0)\n",
    "    anchor = torchvision.ops.boxes.anchor(size, device='cpu')\n",
    "    # Append to final anchors list\n",
    "    anchors.append(anchor)\n",
    "\n",
    "# Concatenate all anchors\n",
    "anchors = torch.cat(anchors, dim=0)\n",
    "\n",
    "# Print sample anchors\n",
    "print(anchors[:6,:])\n",
    "\n",
    "# For example:\n",
    "\"\"\"\n",
    "tensor([[ -64., -64.,  64.,  64.],\n",
    "        [-128., -128., 128., 128.],\n",
    "        [ -256., -256., 256., 256.],\n",
    "        [ -512., -512., 512., 512.],\n",
    "        [ -512., -512., 512., 512.],\n",
    "        [ -512., -512., 512., 512.]])\n",
    "\"\"\"\n",
    "\n",
    "# Define losses\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_bbox = nn.L1Loss()\n",
    "criterion_mask = nn.BCELoss()\n",
    "\n",
    "# Compile losses\n",
    "def loss_fn(outputs, targets):\n",
    "    # Regression and classification losses\n",
    "    loss_cls = criterion_cls(outputs[\"scores\"], targets[\"labels\"])\n",
    "    loss_bbox = criterion_bbox(outputs[\"boxes\"], targets[\"boxes\"])\n",
    "\n",
    "    # Compute mask loss if present\n",
    "    has_mask = targets[\"masks\"] is not None\n",
    "    if has_mask:\n",
    "        loss_mask = criterion_mask(outputs[\"masks\"], targets[\"masks\"])\n",
    "        loss = loss_cls + loss_bbox + loss_mask\n",
    "    else:\n",
    "        loss = loss_cls + loss_bbox\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Train loop\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "def validate(val_loader, model):\n",
    "  model.eval()\n",
    "  val_loss = 0\n",
    "\n",
    "  for inputs, targets in val_loader:\n",
    "      outputs = model(inputs)\n",
    "      val_loss += loss_fn(outputs, targets).item()\n",
    "\n",
    "  print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
    "\n",
    "validate(val_loader, model)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'crack_detector.pth')\n",
    "\n",
    "images, targets = next(iter(dataloader))\n",
    "predictions = model(images)\n",
    "keep = nms(predictions['boxes'], predictions['scores'])\n",
    "masks = predictions['masks'][keep]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
