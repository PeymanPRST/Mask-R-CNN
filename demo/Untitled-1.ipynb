{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peima\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\peima\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to C:\\Users\\peima/.cache\\torch\\hub\\checkpoints\\maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
      "100%|██████████| 170M/170M [00:16<00:00, 10.6MB/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'masks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=102'>103</a>\u001b[0m model \u001b[39m=\u001b[39m MaskRCNN(CrackConfig(), MODEL_DIR)\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=104'>105</a>\u001b[0m \u001b[39m# Convert the mask to binary (0s and 1s)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=105'>106</a>\u001b[0m masks \u001b[39m=\u001b[39m [mask \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m masks]\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=107'>108</a>\u001b[0m \u001b[39m# Convert masks to PyTorch tensors\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=108'>109</a>\u001b[0m masks \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39munsqueeze(mask, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat() \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m masks]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'masks' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import skimage.io\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "import os\n",
    "import json\n",
    "import skimage.io\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# Additional imports\n",
    "import torchvision.utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Set paths\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete')\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\final_models')\n",
    "\n",
    "# Crack dataset class\n",
    "class CrackDataset:\n",
    "    def __init__(self):\n",
    "        self.bbox_data = None\n",
    "\n",
    "    def load_data(self):\n",
    "        IMAGES_DIR = 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\\\\Images'\n",
    "        MASKS_DIR = 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\\\\Masks'\n",
    "        ANNOTATIONS_FILE = 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\\\\BoundingBoxs'\n",
    "\n",
    "        images = os.listdir(os.path.join(DATA_DIR, IMAGES_DIR))\n",
    "        masks = os.listdir(os.path.join(DATA_DIR, MASKS_DIR))\n",
    "\n",
    "        annotations_file_path = os.path.join(DATA_DIR, ANNOTATIONS_FILE)\n",
    "\n",
    "        if not os.path.exists(annotations_file_path):\n",
    "            raise FileNotFoundError(f\"Annotations file '{ANNOTATIONS_FILE}' not found.\")\n",
    "        else:\n",
    "            with open(annotations_file_path) as f:\n",
    "                self.bbox_data = json.load(f)\n",
    "\n",
    "        images_dict = dict(zip(images, masks))\n",
    "        return images_dict\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        image_path = os.path.join(DATA_DIR, 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\\\\Images', image_id)\n",
    "        image = read_image(image_path).float()\n",
    "        return image\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        mask_path = os.path.join(DATA_DIR, 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\\\\Masks', image_id)\n",
    "        mask = read_image(mask_path).float()\n",
    "        return mask\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        return 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\\\\Images' + image_id\n",
    "\n",
    "    def bbox_reference(self, image_id):\n",
    "        if self.bbox_data is not None:\n",
    "            for bb in self.bbox_data:\n",
    "                if bb['image_id'] == image_id:\n",
    "                    return bb  # Return the matching dictionary\n",
    "\n",
    "        return None  # Return None if no matching 'image_id' is found\n",
    "\n",
    "# Crack config\n",
    "class CrackConfig:\n",
    "    # Hyperparameters\n",
    "    NAME = \"crack\"  # Name of your configuration\n",
    "    NUM_CLASSES = 1 + 1  # Crack + Background\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1  # You can adjust this based on your GPU memory\n",
    "\n",
    "# Create crack model\n",
    "class MaskRCNN(nn.Module):\n",
    "    def __init__(self, config, model_dir):\n",
    "        super(MaskRCNN, self).__init__()\n",
    "        self.config = config\n",
    "        self.mask_rcnn = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "    def forward(self, images, masks, targets=None):\n",
    "        images = [F.normalize(image, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)) for image in images]\n",
    "        images = [F.to_tensor(image) for image in images]\n",
    "        images = [F.interpolate(image.unsqueeze(0), size=(256, 256)).squeeze(0) for image in images]\n",
    "        images = torch.stack(images)\n",
    "        return self.mask_rcnn(images)\n",
    "\n",
    "# Train weights on the crack dataset\n",
    "dataset = CrackDataset()\n",
    "model = MaskRCNN(CrackConfig(), MODEL_DIR)\n",
    "\n",
    "# Convert the mask to binary (0s and 1s)\n",
    "masks = [mask > 0.5 for mask in masks]\n",
    "\n",
    "# Convert masks to PyTorch tensors\n",
    "masks = [torch.unsqueeze(mask, dim=0).float() for mask in masks]\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images, masks)\n",
    "    loss = criterion(outputs[0]['masks'].float(), masks[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save trained crack model\n",
    "torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\crack_model.pth'))\n",
    "\n",
    "# Evaluate on the validation dataset\n",
    "val_dataset = CrackDataset()\n",
    "val_images_dict = val_dataset.load_data()\n",
    "val_images, val_masks = list(val_images_dict.keys()), list(val_images_dict.values())\n",
    "\n",
    "val_masks = [mask > 0.5 for mask in val_masks]\n",
    "val_masks = [torch.unsqueeze(mask, dim=0).float() for mask in val_masks]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "for val_image, val_mask in zip(val_images, val_masks):\n",
    "    val_image = dataset.load_image(val_image)\n",
    "    val_image = val_image.unsqueeze(0)\n",
    "    val_mask = val_mask.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_output = model(val_image, None)\n",
    "\n",
    "    loss = criterion(val_output[0]['masks'].float(), val_mask)\n",
    "    total_loss += loss.item()\n",
    "\n",
    "average_loss = total_loss / len(val_images)\n",
    "print(f'Validation Loss: {average_loss}')\n",
    "\n",
    "# Predict crack on a test image (replace 'test_img' with your image data)\n",
    "test_img = None  # Load your test image here\n",
    "\n",
    "# Preprocess the test image\n",
    "test_img = F.normalize(test_img, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "test_img = F.to_tensor(test_img)\n",
    "test_img = F.interpolate(test_img.unsqueeze(0), size=(256, 256)).squeeze(0)\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    results = model(test_img.unsqueeze(0), None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
