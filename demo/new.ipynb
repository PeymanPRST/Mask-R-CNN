{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (service.py, line 86)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3526\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[2], line 7\u001b[0m\n    import tensor\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\peima\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensor\\__init__.py:7\u001b[1;36m\n\u001b[1;33m    from tensor import service\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\peima\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensor\\service.py:86\u001b[1;36m\u001b[0m\n\u001b[1;33m    print \"config:\", repr(config)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
     ]
    }
   ],
   "source": [
    "import os, cv2\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import nms\n",
    "\n",
    "\n",
    "DATA_DIR = \"C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\"\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, \"Images\")\n",
    "MASKS_DIR = os.path.join(DATA_DIR, \"Masks\") \n",
    "ANNOTATIONS_DIR = os.path.join(DATA_DIR, \"BoundingBoxs\")\n",
    "\n",
    "def get_annotations_file_path(image_filename):\n",
    "    filename_without_extension = os.path.splitext(image_filename)[0]\n",
    "    return os.path.join(ANNOTATIONS_DIR, f\"{filename_without_extension}.json\")\n",
    "\n",
    "class CrackDataset:\n",
    "\n",
    "  def __init__(self, root_dir, transform=None):\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "    self.coco = COCO(os.path.join(root_dir, 'annotations.json'))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img = self.load_image(idx)\n",
    "    target = self.load_targets(idx)\n",
    "    if self.transform:\n",
    "      img, target = self.transform(img, target)\n",
    "    return img, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.coco.imgs)\n",
    "\n",
    "  def load_image(self, i):\n",
    "  # get image ID, file name from coco\n",
    "    img_id = self.coco.imgs[i]['id']  \n",
    "    file_name = self.coco.imgs[i]['file_name']\n",
    "\n",
    "  # load and return image\n",
    "    img_path = os.path.join(self.root_dir, 'images', file_name)  \n",
    "    img = cv2.imread(img_path) \n",
    "    return img, img_id\n",
    "\n",
    "  def load_targets(self, img_id):\n",
    "  # get ground truth annotations for image i\n",
    "    ann_ids = self.coco.getAnnIds(imgIds = img_id)  \n",
    "    annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "  # parse bbox and mask from annotations\n",
    "    boxes = []\n",
    "    masks = []\n",
    "    for ann in annotations:\n",
    "      boxes.append(ann['bbox'])\n",
    "    \n",
    "    # load binary mask from COCO polygon area \n",
    "      mask = self.coco.annToMask(ann)  \n",
    "      masks.append(mask)\n",
    "\n",
    "  # construct targets dict   \n",
    "    targets = {'boxes': boxes, 'labels': [], 'masks': masks}  \n",
    "    return targets\n",
    "\n",
    "dataset = CrackDataset(DATA_DIR)\n",
    "dataloader = DataLoader(dataset)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=2)  \n",
    "val_loader = DataLoader(val_set, batch_size=1)\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn()\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005) \n",
    "\n",
    "for epoch in range(30):\n",
    "\n",
    "  model.train()\n",
    "  for imgs, targets in train_loader:\n",
    "\n",
    "    preds = model(imgs)\n",
    "    loss = torch.sum(preds - targets) \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    def evaluate(val_loader, model):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "          for imgs, targets in val_loader:\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, targets)  \n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        return val_loss / len(val_loader)\n",
    "\n",
    "  model.eval()\n",
    "  val_loss = evaluate(val_loader, model)\n",
    "\n",
    "  print(f'Epoch {epoch}, Val Loss: {val_loss}')\n",
    "  \n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(num_classes=2)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Learning rate and optimizer\n",
    "lr = 0.005\n",
    "optimizer = torch.optim.SGD(params, lr=lr, \n",
    "                            momentum=0.9, weight_decay=0.0005)  \n",
    "\n",
    "# Define anchor sizes as a list of lists\n",
    "anchor_sizes = [[128, 128], [256, 256], [512, 512]] \n",
    "\n",
    "# Generate anchors for each size \n",
    "anchors = []\n",
    "for size in anchor_sizes:\n",
    "\n",
    "  # Generate anchors centered at reference point (0,0)\n",
    "  anchor = torchvision.ops.boxes.anchor(size, device='cpu')  \n",
    "\n",
    "  # Append to final anchors list\n",
    "  anchors.append(anchor)\n",
    "\n",
    "# Concatenate all anchors \n",
    "anchors = torch.cat(anchors, dim=0) \n",
    "\n",
    "# Print sample anchors\n",
    "print(anchors[:6,:])\n",
    "\n",
    "# For example:\n",
    "tensor([[ -64., -64.,  64.,  64.],\n",
    "        [-128., -128., 128., 128.],\n",
    "        [ -256., -256., 256., 256.],\n",
    "        [ -512., -512., 512., 512.],\n",
    "        [ -512., -512., 512., 512.],\n",
    "        [ -512., -512., 512., 512.]])\n",
    "\n",
    "# 4. Define losses\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_bbox = nn.L1Loss()\n",
    "criterion_mask = nn.BCELoss()\n",
    "\n",
    "# 5. Compile losses\n",
    "def loss_fn(outputs, targets):\n",
    "  \n",
    "  # Regression and classification losses\n",
    "  loss_cls = criterion_cls(outputs[\"scores\"], targets[\"labels\"])\n",
    "  loss_bbox = criterion_bbox(outputs[\"boxes\"], targets[\"boxes\"])  \n",
    "\n",
    "  # Compute mask loss if present  \n",
    "  has_mask = targets[\"masks\"] is not None\n",
    "  if has_mask:\n",
    "    loss_mask = criterion_mask(outputs[\"masks\"], targets[\"masks\"])\n",
    "    loss = loss_cls + loss_bbox + loss_mask\n",
    "  else:\n",
    "    loss = loss_cls + loss_bbox\n",
    "\n",
    "  return loss\n",
    "\n",
    "# Train loop\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  # Set to training mode\n",
    "  model.train()  \n",
    "\n",
    "  epoch_loss = 0\n",
    "  \n",
    "  for inputs, targets in train_loader:\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)  \n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(outputs, targets)  \n",
    "\n",
    "    # Backward pass and optimize \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  # Print status\n",
    "  print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "\n",
    "def validate(val_loader, model):\n",
    "\n",
    "  # Set to eval mode\n",
    "  model.eval()  \n",
    "\n",
    "  val_loss = 0\n",
    "\n",
    "  for inputs, targets in val_loader:\n",
    "\n",
    "    # Forward pass only\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute validation loss \n",
    "    loss = loss_fn(outputs, targets)  \n",
    "    val_loss += loss.item()\n",
    "\n",
    "  print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
    "  \n",
    "    # Validate at end of each epoch\n",
    "  with torch.no_grad():\n",
    "    validate(val_loader, model)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'crack_detector.pth')\n",
    "\n",
    "images, targets = next(iter(dataloader))\n",
    "predictions = model(images)\n",
    "keep = nms(predictions['boxes'], predictions['scores'])\n",
    "masks = predictions['masks'][keep]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
