{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'out_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=107'>108</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(params, lr\u001b[39m=\u001b[39m\u001b[39m0.005\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m0.0005\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=108'>109</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=110'>111</a>\u001b[0m model \u001b[39m=\u001b[39m MaskRCNN(\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=111'>112</a>\u001b[0m    backbone\u001b[39m=\u001b[39;49mout_channels,\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=112'>113</a>\u001b[0m    pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=113'>114</a>\u001b[0m    num_classes\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mnum_classes  \u001b[39m# Use the number of classes from the dataset\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=114'>115</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=117'>118</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=118'>119</a>\u001b[0m     \u001b[39mfor\u001b[39;00m images, targets \u001b[39min\u001b[39;00m train_dataloader:\n",
      "File \u001b[1;32mc:\\Users\\peima\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\detection\\mask_rcnn.py:212\u001b[0m, in \u001b[0;36mMaskRCNN.__init__\u001b[1;34m(self, backbone, num_classes, min_size, max_size, image_mean, image_std, rpn_anchor_generator, rpn_head, rpn_pre_nms_top_n_train, rpn_pre_nms_top_n_test, rpn_post_nms_top_n_train, rpn_post_nms_top_n_test, rpn_nms_thresh, rpn_fg_iou_thresh, rpn_bg_iou_thresh, rpn_batch_size_per_image, rpn_positive_fraction, rpn_score_thresh, box_roi_pool, box_head, box_predictor, box_score_thresh, box_nms_thresh, box_detections_per_img, box_fg_iou_thresh, box_bg_iou_thresh, box_batch_size_per_image, box_positive_fraction, bbox_reg_weights, mask_roi_pool, mask_head, mask_predictor, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m mask_predictor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_classes should be None when mask_predictor is specified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 212\u001b[0m out_channels \u001b[39m=\u001b[39m backbone\u001b[39m.\u001b[39;49mout_channels\n\u001b[0;32m    214\u001b[0m \u001b[39mif\u001b[39;00m mask_roi_pool \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     mask_roi_pool \u001b[39m=\u001b[39m MultiScaleRoIAlign(featmap_names\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m3\u001b[39m\u001b[39m\"\u001b[39m], output_size\u001b[39m=\u001b[39m\u001b[39m14\u001b[39m, sampling_ratio\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'out_channels'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import os\n",
    "import json\n",
    "from torchvision import io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as T\n",
    "from torchvision.transforms import Resize, RandomCrop, Normalize\n",
    "from torchvision.models.detection import MaskRCNN\n",
    "from torch import nn, device\n",
    "import random\n",
    "\n",
    "\n",
    "DATA_DIR = \"C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\Unity_Generation\\\\Concrete\"\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, \"Images\")\n",
    "MASKS_DIR = os.path.join(DATA_DIR, \"Masks\") \n",
    "ANNOTATIONS_DIR = os.path.join(DATA_DIR, \"BoundingBoxs\")\n",
    "\n",
    "def get_annotations_file_path(image_filename):\n",
    "    filename_without_extension = os.path.splitext(image_filename)[0]\n",
    "    return os.path.join(ANNOTATIONS_DIR, f\"{filename_without_extension}.json\")\n",
    "\n",
    "class CrackDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.images = os.listdir(IMAGES_DIR) \n",
    "        self.masks = os.listdir(MASKS_DIR)\n",
    "        self.annotations = {}\n",
    "        self.class_labels = set()  # Keep track of unique class labels\n",
    "        for image_name in self.images:\n",
    "            annotations_file_path = get_annotations_file_path(image_name)\n",
    "            with open(annotations_file_path) as f:\n",
    "                annotation = json.load(f)\n",
    "                self.annotations[image_name] = annotation\n",
    "                if 'objects' in annotation:\n",
    "                    for obj in annotation['objects']:\n",
    "                        self.class_labels.add(obj['class_label'])\n",
    "                    \n",
    "        self.num_classes = len(self.class_labels)  \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image_name = self.images[i]\n",
    "        mask_name = self.masks[i] \n",
    "        annotations = self.annotations[image_name]\n",
    "        image = io.read_image(os.path.join(IMAGES_DIR, image_name))\n",
    "        mask = io.read_image(os.path.join(MASKS_DIR, mask_name))\n",
    "        return image, mask, annotations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def transform(image, mask):\n",
    "    resized = Resize(256)\n",
    "    image = resized(image)\n",
    "    mask = resized(mask)\n",
    "    random_crop = RandomCrop(224,224)  \n",
    "    image, mask = random_crop(image, mask)\n",
    "    if random.random() < 0.5:\n",
    "        image = T.hflip(image)  \n",
    "        mask = T.hflip(mask)\n",
    "    normalized = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    image = normalized(image)\n",
    "    image = T.to_tensor(image)  \n",
    "    mask = torch.squeeze(mask, dim=0)\n",
    "    return image, mask\n",
    "\n",
    "dataset = CrackDataset(DATA_DIR)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load a pre-trained ResNet model\n",
    "backbone = models.resnet50(pretrained=True)\n",
    "in_features = backbone.fc.in_features\n",
    "num_classes = dataset.num_classes  \n",
    "backbone.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "# Freeze some layers of the backbone model\n",
    "for name, param in backbone.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Create a list of trainable parameters\n",
    "params = [p for p in backbone.parameters() if p.requires_grad]\n",
    "\n",
    "# Initialize the optimizer with trainable parameters\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "model = MaskRCNN(\n",
    "   backbone=backbone,\n",
    "   pretrained=True,\n",
    "   num_classes=dataset.num_classes \n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    for images, targets in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Reshape the outputs\n",
    "        batch_size, num_boxes, num_classes = outputs.shape\n",
    "        outputs = outputs.permute(0, 2, 1).reshape(batch_size * num_boxes, num_classes)\n",
    "        \n",
    "        loss = criterion(outputs, targets)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        for images, targets in val_dataloader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Reshape the outputs\n",
    "            batch_size, num_boxes, num_classes = outputs.shape\n",
    "            outputs = outputs.permute(0, 2, 1).reshape(batch_size * num_boxes, num_classes)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            actual = targets\n",
    "            accuracy = (predictions == actual).float().mean()\n",
    "            val_accuracy += accuracy\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_accuracy /= len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
    "        \n",
    "torch.save(model.state_dict(), 'C:\\\\New folder\\\\Dr. Surya\\\\MaskRCNN\\\\crack_maskrcnn.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
